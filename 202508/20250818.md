# RAG

1.	RAG란?
LLM이 사전에 학습된 정보만으로 답변하는 한계를 넘어, 외부 문서나 데이터베이스에서 관련 정보를 검색해 답변을 생성하는 방식입니다.
2. RAG의 주요 구성 요소
	•	인덱싱: 문서를 임베딩하고 벡터 DB에 저장
	•	검색: 사용자 질문 쿼리에 가장 유사한 문서를 검색
	•	증강: 검색된 문서를 이용해 프롬프트를 구성
	•	생성: 구성된 프롬프트를 LLM에 입력하여 응답 생성  ￼ ￼ ￼
3. RAG가 필요한 이유
	•	최신 정보 반영
	•	사실적 연관성 기반 답변으로 환각(hallucination) 감소
	•	도메인 특화 정보를 활용 가능

### 코드 예제
```python
# pip install langchain openai faiss-cpu tiktoken

from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import OpenAI
from langchain.chains import RetrievalQA

# 1. 문서 준비 (예: 간단한 텍스트 리스트)
docs = [
    "파이썬은 범용 프로그래밍 언어이고, 데이터 과학에 자주 사용된다.",
    "RAG는 외부 지식을 참조하며 텍스트를 생성하는 기법이다.",
    "FAISS는 빠른 유사도 검색을 위한 벡터 검색 라이브러리다."
]

# 2. 임베딩 생성 및 벡터 DB 구성
embeddings = OpenAIEmbeddings()
vectorstore = FAISS.from_texts(docs, embeddings)

# 3. LLM + 검색 인터페이스 구성
llm = OpenAI(temperature=0)
qa = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",
    retriever=vectorstore.as_retriever()
)

# 4. 질문에 대해 RAG 방식으로 답변 생성
query = "RAG가 무엇인지 설명해줘"
result = qa.run(query)
print("답변:", result)
```
